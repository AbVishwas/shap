{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f08abf-0832-499a-a0e3-2dae830434ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "54.178290367126465\n",
    "\n",
    ".values =\n",
    "array([array([ 0.00000000e+00,  0.00000000e+00, -2.87491275e-09,  0.00000000e+00,\n",
    "               0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -6.28688213e-11,\n",
    "              -3.71869646e-10,  0.00000000e+00, -3.71778697e-09, -1.77976744e-09,\n",
    "               0.00000000e+00,  0.00000000e+00,  4.23483471e-10,  0.00000000e+00,\n",
    "              -4.09750101e-09,  0.00000000e+00,  0.00000000e+00,  1.49179868e-09,\n",
    "               5.64455149e-10,  0.00000000e+00,  0.00000000e+00, -4.99596808e-10,\n",
    "               0.00000000e+00, -1.32672540e-09,  3.13275450e-09,  0.00000000e+00,\n",
    "               2.99147966e-10,  0.00000000e+00,  2.93152880e-09,  0.00000000e+00,\n",
    "              -3.15640136e-09,  0.00000000e+00,  0.00000000e+00,  5.66114977e-09,\n",
    "               0.00000000e+00,  4.13820089e-09, -1.09139364e-09,  0.00000000e+00,\n",
    "               0.00000000e+00,  1.93631422e-09, -7.05767889e-10,  0.00000000e+00,\n",
    "              -3.11501935e-10,  5.18298293e-10,  0.00000000e+00,  0.00000000e+00,\n",
    "               0.00000000e+00, -5.13940298e-10,  0.00000000e+00,  0.00000000e+00,\n",
    "               0.00000000e+00,  9.52582013e-10,  0.00000000e+00,  0.00000000e+00,\n",
    "               0.00000000e+00,  2.35169344e-10, -1.00760644e-09,  1.13686838e-09,\n",
    "               0.00000000e+00,  0.00000000e+00,  7.56017471e-10, -1.27988642e-09,\n",
    "               0.00000000e+00,  8.73933459e-09, -6.95308700e-10,  3.10183168e-09,\n",
    "               7.73297870e-10,  1.88038030e-10,  0.00000000e+00,  4.51564119e-10,\n",
    "              -5.22049959e-10,  3.64252628e-10,  0.00000000e+00,  2.71938916e-10,\n",
    "               2.94676283e-10, -1.58252078e-10,  0.00000000e+00, -5.87760951e-10,\n",
    "               7.56926966e-10, -6.58474164e-10,  1.04819264e-09,  0.00000000e+00,\n",
    "              -1.60844138e-09, -2.05182005e-09, -6.73026079e-10, -1.06410880e-09,\n",
    "               2.72848394e-12, -6.01175998e-10,  8.47876436e-10, -2.41698217e-10,\n",
    "               0.00000000e+00,  1.52272150e-09,  1.46474122e-09,  3.17004378e-09,\n",
    "               1.18461685e-09, -1.80307325e-09, -2.38742359e-10, -3.79532139e-10,\n",
    "               0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
    "               0.00000000e+00, -3.64570951e-09,  4.16952793e-10,  6.42424258e-10,\n",
    "              -2.04426425e-10, -7.84811246e-10, -3.13593773e-09,  4.72209649e-09,\n",
    "               0.00000000e+00, -2.66315207e-09,  2.89583113e-09,  8.61473382e-09,\n",
    "               2.95403879e-09,  7.13043846e-10, -1.31694833e-09,  1.54977897e-09,\n",
    "               1.14960130e-09,  1.42212872e-09, -2.98314262e-10, -5.17199321e-10,\n",
    "               8.58135001e-11, -4.48684053e-10, -3.44597437e-11,  1.73496942e-10,\n",
    "               0.00000000e+00])                                                  ,\n",
    "       array([ 0.00000000e+00,  1.15251169e-08,  0.00000000e+00, -5.05679054e-09,\n",
    "               3.42879503e-10,  0.00000000e+00,  0.00000000e+00,  5.98538463e-09,\n",
    "               0.00000000e+00, -6.57928467e-09,  0.00000000e+00,  0.00000000e+00,\n",
    "              -1.67347025e-09, -3.89627530e-09,  1.74259185e-09,  0.00000000e+00,\n",
    "              -3.33784556e-10, -1.58888724e-09,  1.75896275e-09,  0.00000000e+00,\n",
    "              -1.76623871e-09,  1.23691279e-10, -8.34916136e-10,  0.00000000e+00,\n",
    "              -1.77124093e-09,  0.00000000e+00,  0.00000000e+00, -1.34241418e-09,\n",
    "              -2.91402102e-09, -1.15051080e-09, -3.06772563e-09,  3.17413651e-10,\n",
    "               0.00000000e+00,  0.00000000e+00, -7.14317139e-09,  4.99676389e-09,\n",
    "              -2.73075784e-10,  0.00000000e+00,  1.56069291e-09,  1.09139364e-11,\n",
    "              -8.52196536e-10, -4.51018423e-09,  0.00000000e+00,  0.00000000e+00,\n",
    "              -7.26686267e-10,  1.19416654e-09, -2.10548023e-09, -1.19234755e-09,\n",
    "               0.00000000e+00,  5.04587661e-09,  0.00000000e+00,  4.82714313e-09,\n",
    "               0.00000000e+00, -5.30235411e-09,  4.36496824e-09,  4.82941687e-10,\n",
    "               2.66572897e-09,  4.35920811e-09,  0.00000000e+00,  1.37333700e-09,\n",
    "               0.00000000e+00, -6.13636075e-09, -4.22642188e-09, -2.77577783e-09,\n",
    "               1.88265403e-09,  0.00000000e+00, -3.62888386e-09, -3.77743466e-10,\n",
    "              -1.08723595e-09, -3.29600880e-09, -2.72029865e-09,  1.79261406e-09,\n",
    "               1.47338142e-10,  3.58886609e-09, -1.98633643e-09,  0.00000000e+00,\n",
    "               0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
    "              -4.86534191e-09,  1.71121428e-09, -4.48153514e-09, -1.51749191e-09,\n",
    "              -1.40062184e-10, -1.22872734e-09,  0.00000000e+00,  2.09911377e-09,\n",
    "              -8.35825631e-10, -7.97626853e-10,  0.00000000e+00,  3.22961569e-09,\n",
    "               0.00000000e+00, -1.70954687e-09,  9.27684596e-11, -3.81583555e-10,\n",
    "               9.49967216e-10, -1.34984172e-10,  4.96584107e-10, -2.25069622e-09,\n",
    "              -1.44564183e-09,  2.71529643e-09,  1.31071179e-09,  3.45426088e-09,\n",
    "               1.87583282e-10, -9.48299810e-11, -8.85847840e-10,  0.00000000e+00])],\n",
    "      dtype=object)\n",
    "\n",
    ".base_values =\n",
    "array([[0.56289107, 0.4371089 ],\n",
    "       [0.53882831, 0.46117169]])\n",
    "\n",
    ".data =\n",
    "(array(['', 'I ', 'love ', 'sci', '-', 'fi ', 'and ', 'am ', 'willing ',\n",
    "       'to ', 'put ', 'up ', 'with ', 'a ', 'lot', '. ', 'Sci', '-',\n",
    "       'fi ', 'movies', '/', 'TV ', 'are ', 'usually ', 'under', 'fu',\n",
    "       'nded', ', ', 'under', '-', 'appreciated ', 'and ',\n",
    "       'misunderstood', '. ', 'I ', 'tried ', 'to ', 'like ', 'this',\n",
    "       ', ', 'I ', 'really ', 'did', ', ', 'but ', 'it ', 'is ', 'to ',\n",
    "       'good ', 'TV ', 'sci', '-', 'fi ', 'as ', 'Babylon ', '5 ', 'is ',\n",
    "       'to ', 'Star ', 'Trek ', '(', 'the ', 'original', ')', '. ',\n",
    "       'Silly ', 'pro', 'st', 'hetic', 's', ', ', 'cheap ', 'cardboard ',\n",
    "       'sets', ', ', 'stil', 'ted ', 'dialogues', ', ', 'C', 'G ',\n",
    "       'that ', 'doesn', \"'\", 't ', 'match ', 'the ', 'background', ', ',\n",
    "       'and ', 'painfully ', 'one', '-', 'dimensional ', 'characters ',\n",
    "       'cannot ', 'be ', 'overcome ', 'with ', 'a ', \"'\", 'sci', '-',\n",
    "       'fi', \"' \", 'setting', '. ', '(', 'I', \"'\", 'm ', 'sure ',\n",
    "       'there ', 'are ', 'those ', 'of ', 'you ', 'out ', 'there ',\n",
    "       'who ', 'think ', 'Babylon ', '5 ', 'is ', 'good ', 'sci', '-',\n",
    "       'fi', ''], dtype=object), array(['', 'Worth ', 'the ', 'entertainment ', 'value ', 'of ', 'a ',\n",
    "       'rental', ', ', 'especially ', 'if ', 'you ', 'like ', 'action ',\n",
    "       'movies', '. ', 'This ', 'one ', 'features ', 'the ', 'usual ',\n",
    "       'car ', 'chases', ', ', 'fights ', 'with ', 'the ', 'great ',\n",
    "       'Van ', 'Dam', 'me ', 'kick ', 'style', ', ', 'shooting ',\n",
    "       'battles ', 'with ', 'the ', '40 ', 'shell ', 'load ', 'shotgun',\n",
    "       ', ', 'and ', 'even ', 'terrorist ', 'style ', 'bombs', '. ',\n",
    "       'All ', 'of ', 'this ', 'is ', 'entertaining ', 'and ',\n",
    "       'competent', 'ly ', 'handled ', 'but ', 'there ', 'is ',\n",
    "       'nothing ', 'that ', 'really ', 'blows ', 'you ', 'away ', 'if ',\n",
    "       'you', \"'\", 've ', 'seen ', 'your ', 'share ', 'before', '.', '<',\n",
    "       'br ', '/', '>', '<', 'br ', '/', '>', 'The ', 'plot ', 'is ',\n",
    "       'made ', 'interesting ', 'by ', 'the ', 'inclusion ', 'of ', 'a ',\n",
    "       'rabbit', ', ', 'which ', 'is ', 'clever ', 'but ', 'hardly ',\n",
    "       'profound', '. ', 'Many ', 'of ', 'the ', 'c', ''], dtype=object))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34d483a8-5dfc-47f9-970f-1b80feb730a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import queue  # multi-producer, multi-consumer queues\n",
    "import time  # time execution\n",
    "from itertools import chain, combinations, product\n",
    "\n",
    "import numpy as np  # numpy base\n",
    "from numba import njit  # just in time compiler\n",
    "from tqdm.auto import tqdm  # progress bar\n",
    "\n",
    "from shap import Explanation, links  # shap modules\n",
    "from shap.explainers._explainer import Explainer\n",
    "from shap.models import Model\n",
    "from shap.utils import MaskedModel, OpChain, make_masks, safe_isinstance\n",
    "\n",
    "\n",
    "class PartitionExplainer2(Explainer):\n",
    "    \"\"\"Uses the Partition SHAP method to explain the output of any function.\n",
    "    The method has two options:\n",
    "    1. The Cluster method. Users can pass or call a scipy hierarchy of the features using any distance measure.\n",
    "    2. The custom Partition method. Users can pass any nested dictionaries of the feature coalitions.\n",
    "\n",
    "    Partition SHAP computes Shapley values recursively through a hierarchy of features, this\n",
    "    hierarchy defines feature coalitions and results in the Winter values from game theory.\n",
    "\n",
    "    The Cluster method has two particularly nice properties:\n",
    "\n",
    "    1) PartitionExplainer is model-agnostic but when using a balanced partition tree only has\n",
    "       quadratic exact runtime (in term of the number of input features). This is in contrast to the\n",
    "       exponential exact runtime of KernelExplainer or SamplingExplainer.\n",
    "    2) PartitionExplainer always assigns to groups of correlated features the credit that set of features\n",
    "       would have had if treated as a group. This means if the hierarchical clustering given to\n",
    "       PartitionExplainer groups correlated features together, then feature correlations are\n",
    "       \"accounted for\" in the sense that the total credit assigned to a group of tightly dependent features\n",
    "       does not depend on how they behave if their correlation structure was broken during the explanation's\n",
    "       perturbation process.\n",
    "    Note that for linear models the Winter values that PartitionExplainer returns are the same as the standard\n",
    "    non-hierarchical Shapley values.\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        masker,\n",
    "        *,\n",
    "        output_names=None,\n",
    "        link=links.identity,\n",
    "        linearize_link=True,\n",
    "        feature_names=None,\n",
    "        partition_tree=None,\n",
    "        **call_args,\n",
    "    ):\n",
    "        \"\"\"Build a PartitionExplainer for the given model with the given masker.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : function\n",
    "            User supplied function that takes a matrix of samples (# samples x # features) and\n",
    "            computes the output of the model for those samples.\n",
    "\n",
    "        masker : function or numpy.array or pandas.DataFrame or tokenizer\n",
    "            The function used to \"mask\" out hidden features of the form `masker(mask, x)`. It takes a\n",
    "            single input sample and a binary mask and returns a matrix of masked samples. These\n",
    "            masked samples will then be evaluated using the model function and the outputs averaged.\n",
    "            As a shortcut for the standard masking using by SHAP you can pass a background data matrix\n",
    "            instead of a function and that matrix will be used for masking. Domain specific masking\n",
    "            functions are available in shap such as shap.maksers.Image for images and shap.maskers.Text\n",
    "            for text.\n",
    "\n",
    "        partition_tree : Nested dictionary of features  ################################ NOT CURRENTLY IMPLEMENTED #####################################\n",
    "            A hierarchical clustering of the input features represented by a matrix that follows the format\n",
    "            used by scipy.cluster.hierarchy (see the notebooks_html/partition_explainer directory an example).\n",
    "            If this is a function then the function produces a clustering matrix when given a single input\n",
    "            example. If you are using a standard SHAP masker object then you can pass masker.clustering\n",
    "            to use that masker's built-in clustering of the features, or if partition_tree is None then\n",
    "            masker.clustering will be used by default.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        See `Partition explainer examples <https://shap.readthedocs.io/en/latest/api_examples/explainers/PartitionExplainer.html>`_\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            model,\n",
    "            masker,\n",
    "            link=link,\n",
    "            linearize_link=linearize_link,\n",
    "            algorithm=\"partition\",\n",
    "            output_names=output_names,\n",
    "            feature_names=feature_names,\n",
    "        )\n",
    "\n",
    "        # convert dataframes\n",
    "        # if isinstance(masker, pd.DataFrame):\n",
    "        #     masker = TabularMasker(masker)\n",
    "        # elif isinstance(masker, np.ndarray) and len(masker.shape) == 2:\n",
    "        #     masker = TabularMasker(masker)\n",
    "        # elif safe_isinstance(masker, \"transformers.PreTrainedTokenizer\"):\n",
    "        #     masker = TextMasker(masker)\n",
    "        # self.masker = masker\n",
    "\n",
    "        # TODO: maybe? if we have a tabular masker then we build a PermutationExplainer that we\n",
    "        # will use for sampling\n",
    "        self.input_shape = (\n",
    "            masker.shape[1:]\n",
    "            if hasattr(masker, \"shape\") and not callable(masker.shape)\n",
    "            else None\n",
    "        )\n",
    "        # self.output_names = output_names\n",
    "        if not safe_isinstance(self.model, \"shap.models.Model\"):\n",
    "            self.model = Model(self.model)  # lambda *args: np.array(model(*args))\n",
    "        self.expected_value = None\n",
    "        self._curr_base_value = None\n",
    "\n",
    "        # handle higher dimensional tensor inputs\n",
    "        if self.input_shape is not None and len(self.input_shape) > 1:\n",
    "            self._reshaped_model = lambda x: self.model(\n",
    "                x.reshape(x.shape[0], *self.input_shape)\n",
    "            )\n",
    "        else:\n",
    "            self._reshaped_model = self.model\n",
    "\n",
    "        self.partition_tree = partition_tree\n",
    "        if partition_tree is not None:\n",
    "            self.root = Node(\"Root\")\n",
    "            build_tree(partition_tree, self.root)\n",
    "            self.combinations_list = generate_paths_and_combinations(self.root)\n",
    "            self.masks, self.keys = create_masks1(self.root, self.masker.feature_names)\n",
    "            self.masks_dict = dict(zip(self.keys, self.masks))\n",
    "            self.unique_masks = create_combined_masks(\n",
    "                self.combinations_list, self.masks_dict\n",
    "            )\n",
    "            self.unique_masks_list = [mask for _, mask in self.unique_masks]\n",
    "            self.unique_masks_set = set(map(tuple, self.unique_masks_list))\n",
    "            self.unique_masks = [np.array(mask) for mask in self.unique_masks_set]\n",
    "            self._clustering = (\n",
    "                None  # Ensure _clustering is None when using partition_tree\n",
    "            )\n",
    "        else:\n",
    "            if not hasattr(masker, \"clustering\"):\n",
    "                raise ValueError(\n",
    "                    \"The passed masker does not have masker.clustering, so the partition_tree must be passed!\"\n",
    "                )\n",
    "            self._clustering = masker.clustering\n",
    "            if not callable(masker.clustering):\n",
    "                self._mask_matrix = make_masks(self._clustering)\n",
    "\n",
    "        # if getattr(self.masker, \"clustering\", None) is None:\n",
    "        #     raise ValueError(\n",
    "        #         \"The passed masker must have a .clustering attribute defined! Try shap.maskers.Partition(data) for example.\"\n",
    "        #     )\n",
    "\n",
    "        # if partition_tree is None:\n",
    "        #     if not hasattr(masker, \"partition_tree\"):\n",
    "        #         raise ValueError(\n",
    "        #             \"The passed masker does not have masker.clustering, so the partition_tree must be passed!\"\n",
    "        #         )\n",
    "        #     self.partition_tree = masker.clustering\n",
    "        # else:\n",
    "        #     self.partition_tree = partition_tree\n",
    "        # if we don't have a dynamic clustering algorithm then can precowe mpute\n",
    "        # a lot of information\n",
    "        # if not callable(self.masker.clustering):\n",
    "        #     self._clustering = self.masker.clustering  #\n",
    "        #     self._mask_matrix = make_masks(self._clustering)  # make masks argument\n",
    "\n",
    "        # if we have gotten default arguments for the call function we need to wrap ourselves in a new class that\n",
    "        # has a call function with those new default arguments\n",
    "        if len(call_args) > 0:\n",
    "\n",
    "            class PartitionExplainer(self.__class__):\n",
    "                # this signature should match the __call__ signature of the class defined below\n",
    "                def __call__(\n",
    "                    self,\n",
    "                    *args,\n",
    "                    max_evals=500,\n",
    "                    fixed_context=None,\n",
    "                    main_effects=False,\n",
    "                    error_bounds=False,\n",
    "                    batch_size=\"auto\",\n",
    "                    outputs=None,\n",
    "                    silent=False,\n",
    "                ):\n",
    "                    return super().__call__(\n",
    "                        *args,\n",
    "                        max_evals=max_evals,\n",
    "                        fixed_context=fixed_context,\n",
    "                        main_effects=main_effects,\n",
    "                        error_bounds=error_bounds,\n",
    "                        batch_size=batch_size,\n",
    "                        outputs=outputs,\n",
    "                        silent=silent,\n",
    "                    )\n",
    "\n",
    "            PartitionExplainer.__call__.__doc__ = self.__class__.__call__.__doc__\n",
    "            self.__class__ = PartitionExplainer\n",
    "            for k, v in call_args.items():\n",
    "                self.__call__.__kwdefaults__[k] = v\n",
    "\n",
    "    # note that changes to this function signature should be copied to the default call argument wrapper above\n",
    "    def __call__(\n",
    "        self,\n",
    "        *args,\n",
    "        max_evals=500,\n",
    "        fixed_context=None,\n",
    "        main_effects=False,\n",
    "        error_bounds=False,\n",
    "        batch_size=\"auto\",\n",
    "        outputs=None,\n",
    "        silent=False,\n",
    "    ):\n",
    "        \"\"\"Explain the output of the model on the given arguments.\"\"\"\n",
    "        return super().__call__(\n",
    "            *args,\n",
    "            max_evals=max_evals,\n",
    "            fixed_context=fixed_context,\n",
    "            main_effects=main_effects,\n",
    "            error_bounds=error_bounds,\n",
    "            batch_size=batch_size,\n",
    "            outputs=outputs,\n",
    "            silent=silent,\n",
    "        )\n",
    "\n",
    "    def explain_row(\n",
    "        self,\n",
    "        *row_args,\n",
    "        max_evals,\n",
    "        main_effects,\n",
    "        error_bounds,\n",
    "        batch_size,\n",
    "        outputs,\n",
    "        silent,\n",
    "        fixed_context=\"auto\",\n",
    "    ):\n",
    "        if fixed_context == \"auto\":\n",
    "            fixed_context = None\n",
    "        elif fixed_context not in [0, 1, None]:\n",
    "            raise ValueError(\n",
    "                \"Unknown fixed_context value passed (must be 0, 1 or None): %s\"\n",
    "                % fixed_context\n",
    "            )\n",
    "\n",
    "        fm = MaskedModel(\n",
    "            self.model, self.masker, self.link, self.linearize_link, *row_args\n",
    "        )\n",
    "        M = len(fm)\n",
    "        m00 = np.zeros(M, dtype=bool)\n",
    "        if self._curr_base_value is None or not getattr(\n",
    "            self.masker, \"fixed_background\", False\n",
    "        ):\n",
    "            self._curr_base_value = fm(m00.reshape(1, -1), zero_index=0)[0]\n",
    "        f11 = fm(~m00.reshape(1, -1))[0]\n",
    "\n",
    "        if self.partition_tree is not None:\n",
    "            return self.explain_with_partition_tree(\n",
    "                fm,\n",
    "                self._curr_base_value,\n",
    "                f11,\n",
    "                max_evals,\n",
    "                outputs,\n",
    "                fixed_context,\n",
    "                batch_size,\n",
    "                silent,\n",
    "            )\n",
    "        else:\n",
    "            return self.explain_with_clustering(\n",
    "                fm,\n",
    "                self._curr_base_value,\n",
    "                f11,\n",
    "                max_evals,\n",
    "                outputs,\n",
    "                fixed_context,\n",
    "                batch_size,\n",
    "                silent,\n",
    "            )\n",
    "\n",
    "    def explain_with_clustering(\n",
    "        self, fm, f00, f11, max_evals, outputs, fixed_context, batch_size, silent\n",
    "    ):\n",
    "        if callable(self.masker.clustering):\n",
    "            self._clustering = self.masker.clustering(*row_args)\n",
    "            self._mask_matrix = make_masks(self._clustering)\n",
    "        M = len(fm)\n",
    "        m00 = np.zeros(M, dtype=bool)\n",
    "        if (\n",
    "            hasattr(self._curr_base_value, \"shape\")\n",
    "            and len(self._curr_base_value.shape) > 0\n",
    "        ):\n",
    "            if outputs is None:\n",
    "                outputs = np.arange(len(self._curr_base_value))\n",
    "            elif isinstance(outputs, OpChain):\n",
    "                outputs = outputs.apply(Explanation(f11)).values\n",
    "\n",
    "            out_shape = (2 * self._clustering.shape[0] + 1, len(outputs))\n",
    "        else:\n",
    "            out_shape = (2 * self._clustering.shape[0] + 1,)\n",
    "\n",
    "        if max_evals == \"auto\":\n",
    "            max_evals = 500\n",
    "\n",
    "        self.values = np.zeros(out_shape)\n",
    "        self.dvalues = np.zeros(out_shape)\n",
    "\n",
    "        self.owen(\n",
    "            fm,\n",
    "            self._curr_base_value,\n",
    "            f11,\n",
    "            max_evals - 2,\n",
    "            outputs,\n",
    "            fixed_context,\n",
    "            batch_size,\n",
    "            silent,\n",
    "        )\n",
    "        self.values[:] = self.dvalues  # Assign dvalues to values\n",
    "        lower_credit(len(self.dvalues) - 1, 0, M, self.values, self._clustering)\n",
    "        print(self.values[:M].copy())\n",
    "        print(out_shape[1:])\n",
    "        print([s + out_shape[1:] for s in fm.mask_shapes])\n",
    "        return {\n",
    "            \"values\": self.values[:M].copy(),\n",
    "            \"expected_values\": self._curr_base_value\n",
    "            if outputs is None\n",
    "            else self._curr_base_value[outputs],\n",
    "            \"mask_shapes\": [s + out_shape[1:] for s in fm.mask_shapes],\n",
    "            \"main_effects\": None,\n",
    "            \"hierarchical_values\": self.dvalues.copy(),\n",
    "            \"clustering\": self._clustering,\n",
    "            \"output_indices\": outputs,\n",
    "            \"output_names\": getattr(self.model, \"output_names\", None),\n",
    "        }\n",
    "\n",
    "    def explain_with_partition_tree(\n",
    "        self, fm, f00, f11, max_evals, outputs, fixed_context, batch_size, silent\n",
    "    ):\n",
    "        # Step 1: Generate all unique masks\n",
    "        self.root = Node(\"Root\")\n",
    "        build_tree(self.partition_tree, self.root)\n",
    "        self.combinations_list = generate_paths_and_combinations(self.root)\n",
    "        self.masks, self.keys = create_masks1(self.root, self.masker.feature_names)\n",
    "        self.masks_dict = dict(zip(self.keys, self.masks))\n",
    "        self.mask_permutations = create_combined_masks(\n",
    "            self.combinations_list, self.masks_dict\n",
    "        )\n",
    "        self.masks_list = [mask for _, mask in self.mask_permutations]\n",
    "        self.unique_masks_set = set(map(tuple, self.masks_list))\n",
    "        self.unique_masks = [np.array(mask) for mask in self.unique_masks_set]\n",
    "\n",
    "        # if (\n",
    "        #     hasattr(self._curr_base_value, \"shape\")\n",
    "        #     and len(self._curr_base_value.shape) > 0\n",
    "        # ):\n",
    "        #     if outputs is None:\n",
    "        #         outputs = np.arange(len(self._curr_base_value))\n",
    "        #     elif isinstance(outputs, OpChain):\n",
    "        #         outputs = outputs.apply(Explanation(f11)).values\n",
    "\n",
    "        #     out_shape = (2 * self._clustering.shape[0] + 1, len(outputs))\n",
    "        # else:\n",
    "        #     out_shape = (2 * self._clustering.shape[0] + 1,)\n",
    "\n",
    "        # Step 2: Compute model results for all unique masks\n",
    "        mask_results = {}\n",
    "        for mask in self.unique_masks:\n",
    "            # print(mask)\n",
    "            result = fm(mask.reshape(1, -1))\n",
    "            # print(\"model results\", result)\n",
    "            mask_results[tuple(mask)] = result\n",
    "\n",
    "        # Step 3: Compute marginals for permutations\n",
    "        shap_values = np.zeros(len(fm))\n",
    "        last_key_to_off_indexes, last_key_to_on_indexes = (\n",
    "            map_combinations_to_unique_masks(self.mask_permutations, self.unique_masks)\n",
    "        )\n",
    "\n",
    "        feature_name_to_index = {\n",
    "            name: idx for idx, name in enumerate(self.masker.feature_names)\n",
    "        }\n",
    "        # print(feature_name_to_index)\n",
    "\n",
    "        for last_key in last_key_to_off_indexes:\n",
    "            off_indexes = last_key_to_off_indexes[last_key]\n",
    "            on_indexes = last_key_to_on_indexes[last_key]\n",
    "            num_permutations = len(off_indexes)\n",
    "\n",
    "            for off_index, on_index in zip(off_indexes, on_indexes):\n",
    "                off_result = mask_results[tuple(self.unique_masks[off_index])]\n",
    "                on_result = mask_results[tuple(self.unique_masks[on_index])]\n",
    "\n",
    "                # print(\"off\", off_index, off_result,\"on\", on_index, on_result, num_permutations)\n",
    "                # print(\"the values\", (on_result - off_result) / num_permutations)\n",
    "                marginal_contribution = (on_result - off_result) / num_permutations\n",
    "                # print(\"the marginal\", marginal_contribution)\n",
    "                shap_values[feature_name_to_index[last_key]] += (\n",
    "                    marginal_contribution.item()\n",
    "                )\n",
    "\n",
    "        # Step 4: Return results\n",
    "        print(shap_values)\n",
    "        print(len(fm.mask_shapes[0]))\n",
    "        print([s + (len(fm.mask_shapes[0]),) for s in fm.mask_shapes])\n",
    "        return {\n",
    "            \"values\": shap_values.copy(),\n",
    "            \"expected_values\": f00,\n",
    "            \"mask_shapes\": [s + () for s in fm.mask_shapes],\n",
    "            \"main_effects\": None,\n",
    "            \"hierarchical_values\": shap_values,\n",
    "            \"clustering\": None,\n",
    "            \"output_indices\": outputs,\n",
    "            \"output_names\": getattr(self.model, \"output_names\", None),\n",
    "        }\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"shap.explainers.PartitionExplainer()\"\n",
    "\n",
    "    ################\n",
    "    # owen3 has a more efficient strategy dynamically adjusting the context and the number of evaluations\n",
    "    # I removed it for now to understand the method\n",
    "    ###############\n",
    "\n",
    "    ###################### MAIN FUNCTION THAT CALCULATES THE WINTER VALUES #####################\n",
    "\n",
    "    def owen(\n",
    "        self, fm, f00, f11, max_evals, output_indexes, fixed_context, batch_size, silent\n",
    "    ):\n",
    "        \"\"\"Compute a nested set of recursive Owen values based on an ordering recursion.\"\"\"\n",
    "        # f = self._reshaped_model\n",
    "        # r = self.masker\n",
    "        # masks = np.zeros(2*len(inds)+1, dtype=int)\n",
    "        M = len(fm)  # number of features\n",
    "        m00 = np.zeros(\n",
    "            M, dtype=bool\n",
    "        )  # all false mask       #f00 = fm(m00.reshape(1,-1))[0] = fm(m00.reshape(1, -1), zero_index=0)[0]\n",
    "        base_value = f00  # baseline value calculated outside assigneed here only used in the output??????\n",
    "        # f11 = fm(~m00.reshape(1,-1))[0]\n",
    "        # f11 = self._reshaped_model(r(~m00, x)).mean(0)\n",
    "        ind = len(self.dvalues) - 1  # index the length of the hierarchy\n",
    "        # print(\"ind\", ind)\n",
    "\n",
    "        # make sure output_indexes is a list of indexes\n",
    "        if output_indexes is not None:\n",
    "            # assert self.multi_output, \"output_indexes is only valid for multi-output models!\"\n",
    "            # inds = output_indexes.apply(f11, 0)\n",
    "            # out_len = output_indexes_len(output_indexes)\n",
    "            # if output_indexes.startswith(\"max(\"):\n",
    "            #     output_indexes = np.argsort(-f11)[:out_len]\n",
    "            # elif output_indexes.startswith(\"min(\"):\n",
    "            #     output_indexes = np.argsort(f11)[:out_len]\n",
    "            # elif output_indexes.startswith(\"max(abs(\"):\n",
    "            #     output_indexes = np.argsort(np.abs(f11))[:out_len]\n",
    "            # print(output_indexes) # for this one is none\n",
    "\n",
    "            f00 = f00[output_indexes]\n",
    "            f11 = f11[output_indexes]\n",
    "\n",
    "        q = queue.PriorityQueue()  # setting up priority que\n",
    "        q.put(\n",
    "            (0, 0, (m00, f00, f11, ind, 1.0))\n",
    "        )  # the things in the cue are the all false array, all false value, all true value, length of hierarchy, weight used for the division??\n",
    "        eval_count = 0  # starting at 0\n",
    "        total_evals = min(\n",
    "            max_evals, (M - 1) * M\n",
    "        )  # TODO: (M-1)*M is only right for balanced clusterings, but this is just for plotting progress...\n",
    "        pbar = None  # progress bar\n",
    "        start_time = time.time()  # measure time\n",
    "        while not q.empty():  #################### go throught the whole que ##############################\n",
    "            # if we passed our execution limit then leave everything else on the internal nodes\n",
    "            if eval_count >= max_evals:\n",
    "                # print(\"we are doing this\")\n",
    "                while not q.empty():\n",
    "                    m00, f00, f11, ind, weight = q.get()[2]\n",
    "                    self.dvalues[ind] += (f11 - f00) * weight\n",
    "                break\n",
    "\n",
    "            # create a batch of work to do\n",
    "            batch_args = []\n",
    "            batch_masks = []  # batch size is 10 at auto\n",
    "\n",
    "            # this bit creates the entire list of masks needed recursively\n",
    "            while (\n",
    "                not q.empty()\n",
    "                and len(batch_masks) < batch_size\n",
    "                and eval_count + len(batch_masks) < max_evals\n",
    "            ):\n",
    "                # work until q is not empty or other stop criterion are triggered\n",
    "\n",
    "                # get our next set of arguments\n",
    "                m00, f00, f11, ind, weight = q.get()[2]\n",
    "\n",
    "                # get the left and right children of this cluster\n",
    "                # print(ind, M)\n",
    "                lind = int(self._clustering[ind - M, 0]) if ind >= M else -1\n",
    "                rind = int(self._clustering[ind - M, 1]) if ind >= M else -1\n",
    "\n",
    "                # get the distance of this cluster's children\n",
    "                if ind < M:\n",
    "                    distance = -1\n",
    "                else:\n",
    "                    if self._clustering.shape[1] >= 3:\n",
    "                        distance = self._clustering[ind - M, 2]\n",
    "                    else:\n",
    "                        distance = 1\n",
    "\n",
    "                # check if we are a leaf node (or other negative distance cluster) and so should terminate our decent\n",
    "\n",
    "                # here we assign self.dvalues by adding the current marginal ##########################\n",
    "                # it is conditional on the clustering distance\n",
    "                if distance < 0:\n",
    "                    # print(\"the marginals\", (f11 - f00) * weight)\n",
    "                    self.dvalues[ind] += (f11 - f00) * weight\n",
    "                    # print(\"updated values\", self.dvalues, ind)\n",
    "                    continue\n",
    "\n",
    "                # build the masks\n",
    "                # print(self._clustering)\n",
    "                m10 = m00.copy()  # we separate the copy from the add so as to not get converted to a matrix\n",
    "                m10[:] += self._mask_matrix[lind, :]\n",
    "\n",
    "                # print(self._mask_matrix.toarray())\n",
    "                # print(\"the m00\", m00)\n",
    "                # print(\"the left\", lind)\n",
    "                # print(\"what we add\", self._mask_matrix[lind, :].toarray())\n",
    "                # print(\"mask m10\", m10)\n",
    "                m01 = m00.copy()\n",
    "                m01[:] += self._mask_matrix[rind, :]\n",
    "                # print(\"the right\", rind)\n",
    "                # print(\"what we add\", self._mask_matrix[rind, :].toarray())\n",
    "                # print(\"mask m01\", m01)\n",
    "\n",
    "                # add the batch\n",
    "                batch_args.append((m00, m10, m01, f00, f11, ind, lind, rind, weight))\n",
    "                batch_masks.append(m10)\n",
    "                batch_masks.append(m01)\n",
    "\n",
    "            batch_masks = np.array(batch_masks)\n",
    "\n",
    "            # run the batch calculate the\n",
    "            if len(batch_args) > 0:\n",
    "                # print(\"the masks\", batch_masks)\n",
    "                fout = fm(\n",
    "                    batch_masks\n",
    "                )  ######################## call the model using the masks  #############################\n",
    "                if output_indexes is not None:  # making sure the indexing is correct\n",
    "                    fout = fout[:, output_indexes]\n",
    "                # print(\"output of the model\", fout)\n",
    "\n",
    "                eval_count += len(batch_masks)  # update evaluation count\n",
    "                # update the progress bar\n",
    "\n",
    "                if pbar is None and time.time() - start_time > 5:\n",
    "                    pbar = tqdm(total=total_evals, disable=silent, leave=False)\n",
    "                    pbar.update(eval_count)\n",
    "                if pbar is not None:\n",
    "                    pbar.update(len(batch_masks))\n",
    "\n",
    "            # use the results of the batch to add new nodes\n",
    "            # THIS IS HOW THE CODE TRAVERSES THE TREE CREATING ALL POSSIBLE MASKS\n",
    "            # print(batch_args)\n",
    "            for i in range(len(batch_args)):\n",
    "                m00, m10, m01, f00, f11, ind, lind, rind, weight = batch_args[i]\n",
    "\n",
    "                # get the evaluated model output on the two new masked inputs\n",
    "                f10 = fout[2 * i]\n",
    "                f01 = fout[2 * i + 1]\n",
    "\n",
    "                new_weight = weight\n",
    "                if fixed_context is None:\n",
    "                    new_weight /= 2  #\n",
    "                # print(\"new weight\", new_weight)\n",
    "                # elif fixed_context == 0:\n",
    "                #     self.dvalues[ind] += (f11 - f10 - f01 + f00) * weight # leave the interaction effect on the internal node\n",
    "                # elif fixed_context == 1:\n",
    "                #     self.dvalues[ind] -= (f11 - f10 - f01 + f00) * weight # leave the interaction effect on the internal node\n",
    "\n",
    "                ## The idea is to proceed\n",
    "\n",
    "                if fixed_context is None or fixed_context == 0:\n",
    "                    # recurse on the left node with zero context\n",
    "                    args = (m00, f00, f10, lind, new_weight)\n",
    "                    # print(-np.max(np.abs(f10 - f00)) * new_weight)\n",
    "                    q.put(\n",
    "                        (\n",
    "                            -np.max(np.abs(f10 - f00)) * new_weight,\n",
    "                            np.random.randn(),\n",
    "                            args,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    # recurse on the right node with zero context\n",
    "                    args = (m00, f00, f01, rind, new_weight)\n",
    "                    # print(-np.max(np.abs(f01 - f00)) * new_weight)\n",
    "                    q.put(\n",
    "                        (\n",
    "                            -np.max(np.abs(f01 - f00)) * new_weight,\n",
    "                            np.random.randn(),\n",
    "                            args,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                if fixed_context is None or fixed_context == 1:\n",
    "                    # recurse on the left node with one context\n",
    "                    args = (m01, f01, f11, lind, new_weight)\n",
    "                    # print(-np.max(np.abs(f11 - f01)) * new_weight)\n",
    "                    q.put(\n",
    "                        (\n",
    "                            -np.max(np.abs(f11 - f01)) * new_weight,\n",
    "                            np.random.randn(),\n",
    "                            args,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    # recurse on the right node with one context\n",
    "                    args = (m10, f10, f11, rind, new_weight)\n",
    "                    # print(-np.max(np.abs(f11 - f10)) * new_weight)\n",
    "                    q.put(\n",
    "                        (\n",
    "                            -np.max(np.abs(f11 - f10)) * new_weight,\n",
    "                            np.random.randn(),\n",
    "                            args,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        if pbar is not None:\n",
    "            pbar.close()\n",
    "\n",
    "        self.last_eval_count = eval_count\n",
    "\n",
    "        return output_indexes, base_value\n",
    "\n",
    "    ###########################  NOT USED NOW #########################\n",
    "\n",
    "\n",
    "# def output_indexes_len(output_indexes):\n",
    "#     if output_indexes.startswith(\"max(\"):\n",
    "#         return int(output_indexes[4:-1])\n",
    "#     elif output_indexes.startswith(\"min(\"):\n",
    "#         return int(output_indexes[4:-1])\n",
    "#     elif output_indexes.startswith(\"max(abs(\"):\n",
    "#         return int(output_indexes[8:-2])\n",
    "#     elif not isinstance(output_indexes, str):\n",
    "#         return len(output_indexes)\n",
    "\n",
    "\n",
    "@njit\n",
    "def lower_credit(i, value, M, values, clustering):\n",
    "    if i < M:  # M number of features, i\n",
    "        values[i] += value\n",
    "        return\n",
    "    li = int(clustering[i - M, 0])  # get the left index of the top node\n",
    "    ri = int(clustering[i - M, 1])  # get the right index of the top node\n",
    "    group_size = int(clustering[i - M, 3])  # get the number of features in the top node\n",
    "    lsize = int(clustering[li - M, 3]) if li >= M else 1  #\n",
    "    rsize = int(clustering[ri - M, 3]) if ri >= M else 1\n",
    "    assert lsize + rsize == group_size\n",
    "    values[i] += value\n",
    "    lower_credit(li, values[i] * lsize / group_size, M, values, clustering)\n",
    "    lower_credit(ri, values[i] * rsize / group_size, M, values, clustering)\n",
    "\n",
    "\n",
    "# gonna keep the n-ary tree in it's own class for easier referencing\n",
    "class Node:\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        self.child = []\n",
    "        self.permutations = []  # this may not be the greatest idea??\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"({self.key}): {self.child} -> {self.permutations}\"\n",
    "\n",
    "\n",
    "# This function is to encode the dictionary to our specific structure\n",
    "def build_tree(d, root):\n",
    "    if isinstance(d, dict):\n",
    "        for key, value in d.items():\n",
    "            node = Node(key)\n",
    "            root.child.append(node)\n",
    "            build_tree(value, node)\n",
    "    elif isinstance(d, list):\n",
    "        for item in d:\n",
    "            node = Node(item)\n",
    "            root.child.append(node)\n",
    "    # get all the sibling permutations\n",
    "    generate_permutations(root)\n",
    "\n",
    "\n",
    "# generate all permutations of sibling nodes and assign it to the nodes\n",
    "def generate_permutations(node):\n",
    "    if not node.child:  # Leaf node\n",
    "        node.permutations = []\n",
    "        return\n",
    "\n",
    "    children_keys = [child.key for child in node.child]\n",
    "    node.permutations = {}\n",
    "\n",
    "    for i, child in enumerate(node.child):\n",
    "        excluded = children_keys[:i] + children_keys[i + 1 :]\n",
    "        generate_permutations(child)\n",
    "\n",
    "        # Generate all unique combinations of permutations for each child\n",
    "        child.permutations = list(all_subsets(excluded))\n",
    "\n",
    "\n",
    "def all_subsets(iterable):\n",
    "    \"Return all subsets of a given iterable.\"\n",
    "    return chain.from_iterable(\n",
    "        combinations(iterable, n) for n in range(len(iterable) + 1)\n",
    "    )\n",
    "\n",
    "\n",
    "# Functions to generate the base masks for every node in the hierarchy\n",
    "def get_all_leaf_values(node):\n",
    "    leaves = []\n",
    "    if not node.child:\n",
    "        leaves.append(node.key)\n",
    "    else:\n",
    "        for child in node.child:\n",
    "            leaves.extend(get_all_leaf_values(child))\n",
    "    return leaves\n",
    "\n",
    "\n",
    "def create_masks1(node, columns):\n",
    "    masks = [\n",
    "        np.zeros(len(columns), dtype=bool)\n",
    "    ]  # not very efficient for huge trees? maybe??\n",
    "    keys = [()]\n",
    "\n",
    "    if not node.child:  # Check if the child list is empty\n",
    "        mask = columns == node.key\n",
    "        masks.append(mask)\n",
    "        keys.append(node.key)\n",
    "    else:\n",
    "        # Create mask for current node\n",
    "        current_node_mask = columns.isin(get_all_leaf_values(node))\n",
    "        masks.append(current_node_mask)\n",
    "        keys.append(node.key)\n",
    "\n",
    "        # Recursively create masks for all child nodes\n",
    "        for subset in node.child:\n",
    "            child_masks, child_keys = create_masks1(subset, columns)\n",
    "            masks.extend(child_masks)\n",
    "            keys.extend(child_keys)\n",
    "\n",
    "    return masks, keys\n",
    "\n",
    "\n",
    "# combine all the permutations along depth first traversal\n",
    "def generate_paths_and_combinations(node):\n",
    "    paths = []\n",
    "\n",
    "    def dfs(current_node, current_path):\n",
    "        current_path.append((current_node.key, current_node.permutations))\n",
    "\n",
    "        if not current_node.child:  # Leaf node\n",
    "            paths.append(current_path[:])  # Make a copy of current_path\n",
    "        else:\n",
    "            for child in current_node.child:\n",
    "                dfs(child, current_path)\n",
    "\n",
    "        current_path.pop()  # Backtrack\n",
    "\n",
    "    dfs(node, [])\n",
    "\n",
    "    combinations_list = []\n",
    "\n",
    "    for path in paths:\n",
    "        filtered_path = [(key, perms) for key, perms in path if perms]\n",
    "        if filtered_path:\n",
    "            node_keys, permutations = zip(*filtered_path)\n",
    "            path_combinations = list(product(*permutations))\n",
    "            last_key = node_keys[-1]\n",
    "            for combination in path_combinations:\n",
    "                combinations_list.append((last_key, combination))\n",
    "\n",
    "    return combinations_list\n",
    "\n",
    "\n",
    "# functions to combine the masks for all the permutations created\n",
    "def combine_masks(masks):\n",
    "    combined_mask = np.logical_or.reduce(masks)\n",
    "    return combined_mask\n",
    "\n",
    "\n",
    "# def create_combined_masks(combinations, masks_dict):\n",
    "#     combined_masks = []\n",
    "#     for last_key, combination in combinations:\n",
    "#         masks = [masks_dict[key] for keys in combination for key in keys]\n",
    "#         if masks:\n",
    "#             combined_mask = combine_masks(masks)\n",
    "#             combined_masks.append((last_key, combined_mask))\n",
    "\n",
    "#             # Add the combined mask with the last key's mask\n",
    "#             if last_key in masks_dict:\n",
    "#                 combined_mask_with_last_key = combine_masks(masks + [masks_dict[last_key]])\n",
    "#                 combined_masks.append((last_key, combined_mask_with_last_key))\n",
    "#     return combined_masks\n",
    "\n",
    "\n",
    "def create_combined_masks(combinations, masks_dict):\n",
    "    combined_masks = []\n",
    "    for last_key, combination in combinations:\n",
    "        # Collect masks for each key in the combination\n",
    "        masks = []\n",
    "        for keys in combination:\n",
    "            if isinstance(keys, tuple) and not keys:\n",
    "                # Handle empty tuple (())\n",
    "                continue\n",
    "            for key in keys:\n",
    "                if key in masks_dict:\n",
    "                    masks.append(masks_dict[key])\n",
    "\n",
    "        if masks:\n",
    "            # Combine all the masks using logical OR\n",
    "            combined_mask = combine_masks(masks)\n",
    "            combined_masks.append((last_key, combined_mask))\n",
    "\n",
    "            # Add the combined mask with the last key's mask if it's present\n",
    "            if last_key in masks_dict:\n",
    "                combined_mask_with_last_key = combine_masks(\n",
    "                    masks + [masks_dict[last_key]]\n",
    "                )\n",
    "                combined_masks.append((last_key, combined_mask_with_last_key))\n",
    "        else:\n",
    "            # If no masks were found, create a mask of all False values\n",
    "            combined_mask = np.zeros_like(list(masks_dict.values())[0])\n",
    "            combined_masks.append((last_key, combined_mask))\n",
    "\n",
    "            if last_key in masks_dict:\n",
    "                combined_mask_with_last_key = combine_masks(\n",
    "                    [combined_mask, masks_dict[last_key]]\n",
    "                )\n",
    "                combined_masks.append((last_key, combined_mask_with_last_key))\n",
    "    return combined_masks\n",
    "\n",
    "\n",
    "# this may not be needed as we know that all the first values are off and all second values are off\n",
    "def map_combinations_to_unique_masks(combined_masks, unique_masks):\n",
    "    # Create a mapping from mask arrays to their unique index\n",
    "    unique_mask_index_map = {tuple(mask): idx for idx, mask in enumerate(unique_masks)}\n",
    "\n",
    "    # Create dictionaries to hold the mapping of last_key to unique mask indexes for ON and OFF\n",
    "    last_key_to_off_indexes = {}\n",
    "    last_key_to_on_indexes = {}\n",
    "\n",
    "    for i, (last_key, combined_mask) in enumerate(combined_masks):\n",
    "        mask_tuple = tuple(combined_mask)\n",
    "        unique_index = unique_mask_index_map[mask_tuple]\n",
    "\n",
    "        if i % 2 == 0:  # Even index -> OFF value\n",
    "            if last_key not in last_key_to_off_indexes:\n",
    "                last_key_to_off_indexes[last_key] = []\n",
    "            last_key_to_off_indexes[last_key].append(unique_index)\n",
    "        else:  # Odd index -> ON value\n",
    "            if last_key not in last_key_to_on_indexes:\n",
    "                last_key_to_on_indexes[last_key] = []\n",
    "            last_key_to_on_indexes[last_key].append(unique_index)\n",
    "\n",
    "    return last_key_to_off_indexes, last_key_to_on_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687e09f3-5bc6-421e-8808-62368f537ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff4a76-96b2-4d1a-9928-574ff4b3506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model and data\n",
    "model = ResNet50(weights=\"imagenet\")\n",
    "X, y = shap.datasets.imagenet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd13e33-b24f-427a-92f3-c30caeac9865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting ImageNet 1000 class names\n",
    "url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
    "with open(shap.datasets.cache(url)) as file:\n",
    "    class_names = [v[1] for v in json.load(file).values()]\n",
    "# print(\"Number of ImageNet classes:\", len(class_names))\n",
    "# print(\"Class names:\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324dd7c2-47bc-452d-9c9c-58a8fc5661a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "winter_vals",
   "language": "python",
   "name": "winter_vals"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
